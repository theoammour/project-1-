import streamlit as st
import os
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# ==========================================
# Configuration de la page Streamlit
# ==========================================
st.set_page_config(page_title="Docu-Chat ü§ñ", layout="wide")

# CSS personnalis√© pour am√©liorer l'UI
st.markdown("""
<style>
    .main {
        background-color: #f5f5f5;
    }
    h1 {
        color: #2c3e50;
    }
    .stButton>button {
        background-color: #2980b9;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# Titre de l'application
# ==========================================
st.title("üìÑ Docu-Chat : Discutez avec vos PDF")
st.markdown("---")

# ==========================================
# Sidebar : Configuration et Upload
# ==========================================
with st.sidebar:
    st.header("üîß Configuration")
    
    # Champ pour la cl√© API OpenAI (type password pour la s√©curit√©)
    api_key = st.text_input("Cl√© API OpenAI", type="password", help="Entrez votre cl√© sk-...")
    
    st.markdown("---")
    st.header("üìÇ Document")
    
    # Widget d'upload de fichier PDF
    uploaded_file = st.file_uploader("Uploadez votre PDF ici", type="pdf")

# ==========================================
# V√©rification de la Cl√© API
# ==========================================
if not api_key:
    st.warning("‚ö†Ô∏è Veuillez entrer votre cl√© API OpenAI dans la barre lat√©rale pour continuer.")
    st.stop()  # Arr√™te l'ex√©cution du script ici si pas de cl√©

# Configuration de la cl√© pour LangChain
os.environ["OPENAI_API_KEY"] = api_key

# ==========================================
# Logique RAG (Retrieval Augmented Generation)
# ==========================================

@st.cache_resource(show_spinner=False)
def process_pdf(file):
    """
    Fonction pour traiter le PDF : sauvegarde temporaire, chargement, d√©coupage et vectorisation.
    Utilise @st.cache_resource pour ne pas re-calculer √† chaque interaction si le fichier ne change pas.
    """
    with st.spinner("‚è≥ Analyse du document en cours..."):
        try:
            # 1. Sauvegarde du fichier upload√© dans un fichier temporaire
            # Cela est n√©cessaire car PyPDFLoader attend un chemin de fichier
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(file.getvalue())
                tmp_file_path = tmp_file.name

            # 2. Chargement du PDF
            loader = PyPDFLoader(tmp_file_path)
            documents = loader.load()

            # 3. D√©coupage du texte en morceaux (chunks)
            # chunk_size=1000 : taille des morceaux
            # overlap=200 : chevauchement pour garder le contexte entre les morceaux
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_documents(documents)

            # 4. Cr√©ation de la base vectorielle (Embeddings)
            # Utilisation de OpenAIEmbeddings pour transformer le texte en vecteurs
            embeddings = OpenAIEmbeddings()
            
            # Cr√©ation de ChromaDB en m√©moire (pas de persist_directory)
            vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings)

            # Nettoyage du fichier temporaire
            os.remove(tmp_file_path)

            return vectorstore

        except Exception as e:
            st.error(f"Une erreur est survenue lors du traitement du PDF : {e}")
            return None

# Initialisation de la session state pour l'historique (optionnel mais recommand√© pour un chat)
if "messages" not in st.session_state:
    st.session_state.messages = []

# Traitement du fichier si upload√©
if uploaded_file:
    vectorstore = process_pdf(uploaded_file)
    
    if vectorstore:
        st.success("‚úÖ Document charg√© et index√© avec succ√®s !")

        # ==========================================
        # Interface de Chat
        # ==========================================
        
        # Champ de saisie pour la question de l'utilisateur
        query = st.chat_input("Posez votre question sur le document...")

        # Affichage de l'historique des messages (Bonus UX)
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if query:
            # Affichage de la question de l'utilisateur
            with st.chat_message("user"):
                st.markdown(query)
            st.session_state.messages.append({"role": "user", "content": query})

            # G√©n√©ration de la r√©ponse
            with st.chat_message("assistant"):
                with st.spinner("ü§ñ R√©flexion en cours..."):
                    # Cr√©ation du mod√®le de chat
                    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
                    
                    # Cha√Æne de RetrievalQA
                    # chain_type="stuff" : ins√®re tous les documents pertinents dans le prompt
                    qa_chain = RetrievalQA.from_chain_type(
                        llm=llm,
                        chain_type="stuff",
                        retriever=vectorstore.as_retriever(),
                        return_source_documents=True # Important pour r√©cup√©rer les sources
                    )

                    # Ex√©cution de la cha√Æne
                    result = qa_chain.invoke({"query": query})
                    answer = result["result"]
                    source_documents = result["source_documents"]

                    # Affichage de la r√©ponse
                    st.markdown(answer)
                    
                    # Affichage des sources dans un expander
                    with st.expander("üìö Sources utilis√©es"):
                        for i, doc in enumerate(source_documents):
                            st.markdown(f"**Source {i+1} :** (Page {doc.metadata.get('page', 'N/A') + 1})")
                            st.text(doc.page_content[:500] + "...") # Affiche les 500 premiers caract√®res du chunk
            
            # Ajout de la r√©ponse √† l'historique
            st.session_state.messages.append({"role": "assistant", "content": answer})

else:
    st.info("üëÜ Veuillez uploader un fichier PDF dans la barre lat√©rale pour commencer.")

# Footer
st.markdown("---")
st.caption("D√©velopp√© pour Portfolio GitHub - RAG avec LangChain & Streamlit")
